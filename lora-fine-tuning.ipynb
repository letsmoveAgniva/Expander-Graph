{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-15T16:22:39.497402Z","iopub.execute_input":"2024-06-15T16:22:39.497869Z","iopub.status.idle":"2024-06-15T16:22:40.797984Z","shell.execute_reply.started":"2024-06-15T16:22:39.497822Z","shell.execute_reply":"2024-06-15T16:22:40.797066Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchvision.datasets as datasets \nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-06-15T16:22:40.799890Z","iopub.execute_input":"2024-06-15T16:22:40.800830Z","iopub.status.idle":"2024-06-15T16:22:47.739904Z","shell.execute_reply.started":"2024-06-15T16:22:40.800790Z","shell.execute_reply":"2024-06-15T16:22:47.738861Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"_ = torch.manual_seed(0)","metadata":{"execution":{"iopub.status.busy":"2024-06-15T16:22:47.741399Z","iopub.execute_input":"2024-06-15T16:22:47.741915Z","iopub.status.idle":"2024-06-15T16:22:47.749413Z","shell.execute_reply.started":"2024-06-15T16:22:47.741879Z","shell.execute_reply":"2024-06-15T16:22:47.748580Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n\n# Load the MNIST dataset\nmnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n# Create a dataloader for the training\ntrain_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n\n# Load the MNIST test set\nmnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\ntest_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=10, shuffle=True)\n\n# Define the device\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-06-15T16:22:47.752055Z","iopub.execute_input":"2024-06-15T16:22:47.752439Z","iopub.status.idle":"2024-06-15T16:22:50.231879Z","shell.execute_reply.started":"2024-06-15T16:22:47.752405Z","shell.execute_reply":"2024-06-15T16:22:50.230908Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 9912422/9912422 [00:00<00:00, 32820926.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 28881/28881 [00:00<00:00, 1099564.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1648877/1648877 [00:00<00:00, 9319352.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4542/4542 [00:00<00:00, 2578926.33it/s]","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"class RichBoyNet(nn.Module):\n    def __init__(self, hidden_size_1=1000, hidden_size_2=2000):\n        super(RichBoyNet,self).__init__()\n        self.linear1 = nn.Linear(28*28, hidden_size_1) \n        self.linear2 = nn.Linear(hidden_size_1, hidden_size_2) \n        self.linear3 = nn.Linear(hidden_size_2, 10)\n        self.relu = nn.ReLU()\n\n    def forward(self, img):\n        x = img.view(-1, 28*28)\n        x = self.relu(self.linear1(x))\n        x = self.relu(self.linear2(x))\n        x = self.linear3(x)\n        return x\n\nnet = RichBoyNet().to(device)","metadata":{"execution":{"iopub.status.busy":"2024-06-15T16:22:50.233319Z","iopub.execute_input":"2024-06-15T16:22:50.233650Z","iopub.status.idle":"2024-06-15T16:22:50.479400Z","shell.execute_reply.started":"2024-06-15T16:22:50.233622Z","shell.execute_reply":"2024-06-15T16:22:50.478583Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def train(train_loader, net, epochs=5, total_iterations_limit=None):\n    cross_el = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n\n    total_iterations = 0\n\n    for epoch in range(epochs):\n        net.train()\n\n        loss_sum = 0\n        num_iterations = 0\n\n        data_iterator = tqdm(train_loader, desc=f'Epoch {epoch+1}')\n        if total_iterations_limit is not None:\n            data_iterator.total = total_iterations_limit\n        for data in data_iterator:\n            num_iterations += 1\n            total_iterations += 1\n            x, y = data\n            x = x.to(device)\n            y = y.to(device)\n            optimizer.zero_grad()\n            output = net(x.view(-1, 28*28))\n            loss = cross_el(output, y)\n            loss_sum += loss.item()\n            avg_loss = loss_sum / num_iterations\n            data_iterator.set_postfix(loss=avg_loss)\n            loss.backward()\n            optimizer.step()\n\n            if total_iterations_limit is not None and total_iterations >= total_iterations_limit:\n                return\n\ntrain(train_loader, net, epochs=1)","metadata":{"execution":{"iopub.status.busy":"2024-06-15T16:22:50.480720Z","iopub.execute_input":"2024-06-15T16:22:50.481376Z","iopub.status.idle":"2024-06-15T16:23:23.124515Z","shell.execute_reply.started":"2024-06-15T16:22:50.481338Z","shell.execute_reply":"2024-06-15T16:23:23.123444Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"Epoch 1: 100%|██████████| 6000/6000 [00:32<00:00, 183.88it/s, loss=0.234]\n","output_type":"stream"}]},{"cell_type":"code","source":"original_weights = {}\nfor name, param in net.named_parameters():\n    original_weights[name] = param.clone().detach()","metadata":{"execution":{"iopub.status.busy":"2024-06-15T16:23:23.125920Z","iopub.execute_input":"2024-06-15T16:23:23.126240Z","iopub.status.idle":"2024-06-15T16:23:23.132763Z","shell.execute_reply.started":"2024-06-15T16:23:23.126210Z","shell.execute_reply":"2024-06-15T16:23:23.131977Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def test():\n    correct = 0\n    total = 0\n\n    wrong_counts = [0 for i in range(10)]\n\n    with torch.no_grad():\n        for data in tqdm(test_loader, desc='Testing'):\n            x, y = data\n            x = x.to(device)\n            y = y.to(device)\n            output = net(x.view(-1, 784))\n            for idx, i in enumerate(output):\n                if torch.argmax(i) == y[idx]:\n                    correct +=1\n                else:\n                    wrong_counts[y[idx]] +=1\n                total +=1\n    print(f'Accuracy: {round(correct/total, 3)}')\n    for i in range(len(wrong_counts)):\n        print(f'wrong counts for the digit {i}: {wrong_counts[i]}')\n\ntest()","metadata":{"execution":{"iopub.status.busy":"2024-06-15T16:23:23.133861Z","iopub.execute_input":"2024-06-15T16:23:23.134232Z","iopub.status.idle":"2024-06-15T16:23:26.445992Z","shell.execute_reply.started":"2024-06-15T16:23:23.134181Z","shell.execute_reply":"2024-06-15T16:23:26.445072Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"Testing: 100%|██████████| 1000/1000 [00:03<00:00, 303.41it/s]","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.961\nwrong counts for the digit 0: 11\nwrong counts for the digit 1: 20\nwrong counts for the digit 2: 54\nwrong counts for the digit 3: 68\nwrong counts for the digit 4: 24\nwrong counts for the digit 5: 18\nwrong counts for the digit 6: 40\nwrong counts for the digit 7: 55\nwrong counts for the digit 8: 9\nwrong counts for the digit 9: 87\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"total_parameters_original = 0\nfor index, layer in enumerate([net.linear1, net.linear2, net.linear3]):\n    total_parameters_original += layer.weight.nelement() + layer.bias.nelement()\n    print(f'Layer {index+1}: W: {layer.weight.shape} + B: {layer.bias.shape}')\nprint(f'Total number of parameters: {total_parameters_original:,}')","metadata":{"execution":{"iopub.status.busy":"2024-06-15T16:23:26.447126Z","iopub.execute_input":"2024-06-15T16:23:26.447454Z","iopub.status.idle":"2024-06-15T16:23:26.453890Z","shell.execute_reply.started":"2024-06-15T16:23:26.447427Z","shell.execute_reply":"2024-06-15T16:23:26.452917Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Layer 1: W: torch.Size([1000, 784]) + B: torch.Size([1000])\nLayer 2: W: torch.Size([2000, 1000]) + B: torch.Size([2000])\nLayer 3: W: torch.Size([10, 2000]) + B: torch.Size([10])\nTotal number of parameters: 2,807,010\n","output_type":"stream"}]},{"cell_type":"code","source":"class LoRAParametrization(nn.Module):\n    def __init__(self, features_in, features_out, rank=2, alpha=1, device='cpu'):\n        super().__init__()\n        # Section 4.1 of the paper: \n        #   We use a random Gaussian initialization for A and zero for B, so ∆W = BA is zero at the beginning of training\n        self.lora_A = nn.Parameter(torch.zeros((rank,features_out)).to(device))\n        self.lora_B = nn.Parameter(torch.zeros((features_in, rank)).to(device))\n        nn.init.normal_(self.lora_A, mean=0, std=1)\n        \n        # Section 4.1 of the paper: \n        #   We then scale ∆Wx by α/r , where α is a constant in r. \n        #   When optimizing with Adam, tuning α is roughly the same as tuning the learning rate if we scale the initialization appropriately. \n        #   As a result, we simply set α to the first r we try and do not tune it. \n        #   This scaling helps to reduce the need to retune hyperparameters when we vary r.\n        self.scale = alpha / rank\n        self.enabled = True\n\n    def forward(self, original_weights):\n        if self.enabled:\n            # Return W + (B*A)*scale\n            return original_weights + torch.matmul(self.lora_B, self.lora_A).view(original_weights.shape) * self.scale\n        else:\n            return original_weights","metadata":{"execution":{"iopub.status.busy":"2024-06-15T16:26:52.887799Z","iopub.execute_input":"2024-06-15T16:26:52.888202Z","iopub.status.idle":"2024-06-15T16:26:52.896770Z","shell.execute_reply.started":"2024-06-15T16:26:52.888159Z","shell.execute_reply":"2024-06-15T16:26:52.895648Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"import torch.nn.utils.parametrize as parametrize\n\ndef linear_layer_parameterization(layer, device, rank=2, lora_alpha=1):\n\n    \n    features_in, features_out = layer.weight.shape\n    return LoRAParametrization(\n        features_in, features_out, rank=rank, alpha=lora_alpha, device=device\n    )\n\nparametrize.register_parametrization(\n    net.linear1, \"weight\", linear_layer_parameterization(net.linear1, device)\n)\nparametrize.register_parametrization(\n    net.linear2, \"weight\", linear_layer_parameterization(net.linear2, device)\n)\nparametrize.register_parametrization(\n    net.linear3, \"weight\", linear_layer_parameterization(net.linear3, device)\n)\n\n\ndef enable_disable_lora(enabled=True):\n    for layer in [net.linear1, net.linear2, net.linear3]:\n        layer.parametrizations[\"weight\"][0].enabled = enabled","metadata":{"execution":{"iopub.status.busy":"2024-06-15T16:34:50.355042Z","iopub.execute_input":"2024-06-15T16:34:50.355423Z","iopub.status.idle":"2024-06-15T16:34:50.367963Z","shell.execute_reply.started":"2024-06-15T16:34:50.355392Z","shell.execute_reply":"2024-06-15T16:34:50.367008Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"total_parameters_lora = 0\ntotal_parameters_non_lora = 0\nfor index, layer in enumerate([net.linear1, net.linear2, net.linear3]):\n    total_parameters_lora += layer.parametrizations[\"weight\"][0].lora_A.nelement() + layer.parametrizations[\"weight\"][0].lora_B.nelement()\n    total_parameters_non_lora += layer.weight.nelement() + layer.bias.nelement()\n    print(\n        f'Layer {index+1}: W: {layer.weight.shape} + B: {layer.bias.shape} + Lora_A: {layer.parametrizations[\"weight\"][0].lora_A.shape} + Lora_B: {layer.parametrizations[\"weight\"][0].lora_B.shape}'\n    )\n# The non-LoRA parameters count must match the original network\nassert total_parameters_non_lora == total_parameters_original\nprint(f'Total number of parameters (original): {total_parameters_non_lora:,}')\nprint(f'Total number of parameters (original + LoRA): {total_parameters_lora + total_parameters_non_lora:,}')\nprint(f'Parameters introduced by LoRA: {total_parameters_lora:,}')\nparameters_incremment = (total_parameters_lora / total_parameters_non_lora) * 100\nprint(f'Parameters incremment: {parameters_incremment:.3f}%')","metadata":{"execution":{"iopub.status.busy":"2024-06-15T16:35:02.807645Z","iopub.execute_input":"2024-06-15T16:35:02.808011Z","iopub.status.idle":"2024-06-15T16:35:02.820455Z","shell.execute_reply.started":"2024-06-15T16:35:02.807981Z","shell.execute_reply":"2024-06-15T16:35:02.819361Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Layer 1: W: torch.Size([1000, 784]) + B: torch.Size([1000]) + Lora_A: torch.Size([1, 784]) + Lora_B: torch.Size([1000, 1])\nLayer 2: W: torch.Size([2000, 1000]) + B: torch.Size([2000]) + Lora_A: torch.Size([1, 1000]) + Lora_B: torch.Size([2000, 1])\nLayer 3: W: torch.Size([10, 2000]) + B: torch.Size([10]) + Lora_A: torch.Size([1, 2000]) + Lora_B: torch.Size([10, 1])\nTotal number of parameters (original): 2,807,010\nTotal number of parameters (original + LoRA): 2,813,804\nParameters introduced by LoRA: 6,794\nParameters incremment: 0.242%\n","output_type":"stream"}]},{"cell_type":"code","source":"for name, param in net.named_parameters():\n    if 'lora' not in name:\n        print(f'Freezing non-LoRA parameter {name}')\n        param.requires_grad = False\n\n# Load the MNIST dataset again, by keeping only the digit 9\nmnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\nexclude_indices = mnist_trainset.targets == 9\nmnist_trainset.data = mnist_trainset.data[exclude_indices]\nmnist_trainset.targets = mnist_trainset.targets[exclude_indices]\n# Create a dataloader for the training\ntrain_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n\n# Train the network with LoRA only on the digit 9 and only for 100 batches (hoping that it would improve the performance on the digit 9)\ntrain(train_loader, net, epochs=1, total_iterations_limit=100)","metadata":{"execution":{"iopub.status.busy":"2024-06-15T16:23:26.583061Z","iopub.execute_input":"2024-06-15T16:23:26.583394Z","iopub.status.idle":"2024-06-15T16:23:27.301250Z","shell.execute_reply.started":"2024-06-15T16:23:26.583357Z","shell.execute_reply":"2024-06-15T16:23:27.300390Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Freezing non-LoRA parameter linear1.bias\nFreezing non-LoRA parameter linear1.parametrizations.weight.original\nFreezing non-LoRA parameter linear2.bias\nFreezing non-LoRA parameter linear2.parametrizations.weight.original\nFreezing non-LoRA parameter linear3.bias\nFreezing non-LoRA parameter linear3.parametrizations.weight.original\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1:  99%|█████████▉| 99/100 [00:00<00:00, 164.01it/s, loss=0.0961]\n","output_type":"stream"}]},{"cell_type":"code","source":"assert torch.all(net.linear1.parametrizations.weight.original == original_weights['linear1.weight'])\nassert torch.all(net.linear2.parametrizations.weight.original == original_weights['linear2.weight'])\nassert torch.all(net.linear3.parametrizations.weight.original == original_weights['linear3.weight'])\n\nenable_disable_lora(enabled=True)\n# The new linear1.weight is obtained by the \"forward\" function of our LoRA parametrization\n# The original weights have been moved to net.linear1.parametrizations.weight.original\n# More info here: https://pytorch.org/tutorials/intermediate/parametrizations.html#inspecting-a-parametrized-module\nassert torch.equal(net.linear1.weight, net.linear1.parametrizations.weight.original + (net.linear1.parametrizations.weight[0].lora_B @ net.linear1.parametrizations.weight[0].lora_A) * net.linear1.parametrizations.weight[0].scale)\n\nenable_disable_lora(enabled=False)\n# If we disable LoRA, the linear1.weight is the original one\nassert torch.equal(net.linear1.weight, original_weights['linear1.weight'])","metadata":{"execution":{"iopub.status.busy":"2024-06-15T16:23:27.302513Z","iopub.execute_input":"2024-06-15T16:23:27.302871Z","iopub.status.idle":"2024-06-15T16:23:27.341390Z","shell.execute_reply.started":"2024-06-15T16:23:27.302836Z","shell.execute_reply":"2024-06-15T16:23:27.340594Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Test with LoRA enabled\nenable_disable_lora(enabled=True)\ntest()","metadata":{"execution":{"iopub.status.busy":"2024-06-15T16:23:27.342627Z","iopub.execute_input":"2024-06-15T16:23:27.342976Z","iopub.status.idle":"2024-06-15T16:23:30.957449Z","shell.execute_reply.started":"2024-06-15T16:23:27.342945Z","shell.execute_reply":"2024-06-15T16:23:30.956558Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"Testing: 100%|██████████| 1000/1000 [00:03<00:00, 277.30it/s]","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.894\nwrong counts for the digit 0: 14\nwrong counts for the digit 1: 23\nwrong counts for the digit 2: 65\nwrong counts for the digit 3: 130\nwrong counts for the digit 4: 330\nwrong counts for the digit 5: 162\nwrong counts for the digit 6: 54\nwrong counts for the digit 7: 195\nwrong counts for the digit 8: 74\nwrong counts for the digit 9: 14\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"# Test with LoRA disabled\nenable_disable_lora(enabled=False)\ntest()","metadata":{"execution":{"iopub.status.busy":"2024-06-15T18:16:31.048099Z","iopub.execute_input":"2024-06-15T18:16:31.048670Z","iopub.status.idle":"2024-06-15T18:16:31.374043Z","shell.execute_reply.started":"2024-06-15T18:16:31.048640Z","shell.execute_reply":"2024-06-15T18:16:31.372829Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Test with LoRA disabled\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43menable_disable_lora\u001b[49m(enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m test()\n","\u001b[0;31mNameError\u001b[0m: name 'enable_disable_lora' is not defined"],"ename":"NameError","evalue":"name 'enable_disable_lora' is not defined","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}